python run_clm.py --config_name configs/gpt2_small.json --model_type gpt2 --tokenizer_name gpt2 --dataset_name wikitext --dataset_cache_dir /cb/ml/language/datasets/huggingface/ --dataset_config_name wikitext-2-raw-v1 --dataset_intmd_dir /cb/ml/language/datasets/huggingface/wikitext/ --do_train --do_eval --per_device_train_batch_size 9 --gradient_accumulation_steps 4 --fp16 --dataloader_num_workers 8 --preprocessing_num_workers 8 --overwrite_output_dir --block_size 512 --evaluation_strategy steps --eval_steps 150 --save_steps 1000 --logging_dir runs --learning_rate 2.8e-4 --max_grad_norm 0.0 --lr_scheduler_type constant --adam_epsilon 1e-6 --output_dir ./gpt2_small --logging_steps 50
# l1
python run_clm.py --config_name configs/gpt2_small.json --model_type gpt2 --tokenizer_name gpt2 --dataset_name wikitext --dataset_cache_dir /cb/ml/language/datasets/huggingface/ --dataset_config_name wikitext-2-raw-v1 --dataset_intmd_dir /cb/ml/language/datasets/huggingface/wikitext/ --do_train --do_eval --per_device_train_batch_size 9 --gradient_accumulation_steps 4 --per_device_eval_batch_size 9 --fp16 --dataloader_num_workers 8 --preprocessing_num_workers 8 --overwrite_output_dir --block_size 512 --evaluation_strategy steps --eval_steps 150 --save_steps 1000 --logging_dir runs --learning_rate 2.8e-4 --max_grad_norm 0.0 --lr_scheduler_type constant --adam_epsilon 1e-6 --reg_type l1 --output_dir ./gpt2_small_l1
# hoyer_sq
python run_clm.py --config_name configs/gpt2_small.json --model_type gpt2 --tokenizer_name gpt2 --dataset_name wikitext --dataset_cache_dir /cb/ml/language/datasets/huggingface/ --dataset_config_name wikitext-2-raw-v1 --dataset_intmd_dir /cb/ml/language/datasets/huggingface/wikitext/ --do_train --do_eval --per_device_train_batch_size 9 --gradient_accumulation_steps 4 --fp16 --dataloader_num_workers 8 --preprocessing_num_workers 8 --overwrite_output_dir --block_size 512 --evaluation_strategy steps --eval_steps 150 --save_steps 1000 --logging_dir runs --learning_rate 2.8e-4 --max_grad_norm 0.0 --lr_scheduler_type constant --adam_epsilon 1e-6 --reg_type hoyer_sq --output_dir ./gpt2_small_hoyer_sq



python run_clm.py --config_name configs/gpt2_small.json --model_type gpt2 --tokenizer_name gpt2 --dataset_name openwebtext --dataset_cache_dir /cb/ml/language/datasets/huggingface/ --dataset_intmd_dir /cb/ml/language/datasets/huggingface/openwebtext/cache_dir --do_train --do_eval --per_device_train_batch_size 9 --gradient_accumulation_steps 4 --fp16 --dataloader_num_workers 8 --preprocessing_num_workers 8 --overwrite_output_dir --block_size 512 --evaluation_strategy steps --eval_steps 1000 --save_steps 1000 --logging_dir runs --learning_rate 2.8e-4 --max_grad_norm 0.0 --lr_scheduler_type constant --adam_epsilon 1e-6 --output_dir ./gpt2_small --save_total_limit 2 --logging_strategy steps --logging_steps 500


# rigl w/ wikitext
python run_clm.py --config_name configs/gpt2_small.json --model_type gpt2 --tokenizer_name gpt2 --dataset_name wikitext --dataset_cache_dir /cb/ml/language/datasets/huggingface/ --dataset_config_name wikitext-2-raw-v1 --dataset_intmd_dir /cb/ml/language/datasets/huggingface/wikitext/ --do_train --do_eval --per_device_train_batch_size 9 --gradient_accumulation_steps 4 --fp16 --dataloader_num_workers 8 --preprocessing_num_workers 8 --overwrite_output_dir --block_size 512 --evaluation_strategy steps --eval_steps 150 --save_steps 1000 --logging_dir runs --learning_rate 2.8e-4 --max_grad_norm 0.0 --lr_scheduler_type constant --adam_epsilon 1e-6 --output_dir ./gpt2_small --logging_steps 50 --rigl --rigl_erk_power_scale 4.0 --rigl_growth_frequency 500
